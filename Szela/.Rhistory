#1.3
plot(orange.tree)
text(orange.tree, pretty = 0)
summary(orange.tree)
#Number of terminal nodes is 86
predictions = predict(orange.tree, test.OJ, class = 'type')
predictions
#1.4
table(predictions, test.OJ.target)
predictions = predict(orange.tree, test.OJ, type = "class")
predictions
#1.4
table(predictions, test.OJ.target)
#Assessing the accuracy of the overall tree by constructing a confusion matrix.
table(tree.pred, High.test)
library(tree)
#Loading the ISLR library in order to use the Carseats dataset.
library(ISLR)
#Making data manipulation easier.
help(Carseats)
attach(Carseats)
#Looking at the variable of interest, Sales.
hist(Sales)
summary(Sales)
#Creating a binary categorical variable High based on the continuous Sales
#variable and adding it to the original data frame.
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
#Fit a tree to the data; note that we are excluding Sales from the formula.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
#The output shows the variables actually used within the tree, the number of
#terminal nodes, the residual mean deviance based on the Gini index, and
#the misclassification error rate.
#Plotting the classification tree.
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
#Detailed information for the splits of the classification tree.
tree.carseats
#The output shows the variables used at each node, the split rule, the number
#of observations at each node, the deviance based on the Gini index, the
#majority class value based on the observations in the node, and the associated
#probabilities of class membership at each node. Terminal nodes are denoted
#by asterisks.
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
#Ftting and visualizing a classification tree to the training data.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
#Using the trained decision tree to classify the test data.
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
#Assessing the accuracy of the overall tree by constructing a confusion matrix.
table(tree.pred, High.test)
(60 + 42)/120
table(predictions, test.OJ.target)
(106 + 59) / length(test.OJ.target)
?cv.tree
set.seed(0)
set.seed(0)
orange.cv = cv.tree(orange.tree, FUN = prune.tree)
names(cv.carseats)
names(orange.cv)
summary(orange.tree)
train = read.csv("training.csv")
library(xgboost)
library(caret)
library(neuralnet)
setwd("~/GitHub-kszela24/Higgs")
train = read.csv("training.csv")
test = read.csv("test.csv")
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
train[train == -999] = NA
test[test == -999] = NA
train.ID <- train$EventId
train$EventId = NULL
train.weight <- train$Weight
train$Weight = NULL
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain < xgb.DMatrix(data = train.model, label = train$Label)
dtrain <- xgb.DMatrix(data = train.model, label = train$Label)
length(train$Label)
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train$Label)
length(train.model)
train.y <- train$Label
train.model <- sparse.model.matrix(Label ~ ., data = train)
train.blah <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data=train, label=train.y)
dtrain <- xgb.DMatrix(data=train.blah, label=train.y)
library(xgboost)
library(caret)
library(neuralnet)
setwd("~/GitHub-kszela24/Higgs")
train = read.csv("training.csv")
test = read.csv("test.csv")
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
train.ID <- train$EventId
train$EventId = NULL
train.weight <- train$Weight
train$Weight = NULL
train.y <- train$Label
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
watchlist <- list(train=dtrain)
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = "auc",
eta                 = 0.02,
max_depth           = 5,
subsample           = 0.6815,
colsample_bytree    = 0.701
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 560,
nfolds              = 5,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 200,
nfolds              = 5,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
test.ID <- test$EventId
test$EventId <- NULL
test$Label <- -1
test <- sparse.model.matrix(Label ~ ., data = test)
preds <- predict(clf, test)
?num
?count
nrow(preds[preds > 0.5])
preds[preds > 0.5]
count(preds[preds > 0.5])
?num
dim(preds[preds > 0.5])
length(preds[preds > 0.5])
length(preds[preds <= 0.5])
length(train.y[train.y == 1])
length(train.y[train.y == 0])
length(train.y[train.y == 1]) / length(train.y[train.y == 0])
train = read.csv("training.csv")
preds.num.s = length(preds[preds > 0.5])
preds.num.b = length(preds[preds <= 0.5])
train.y.s = length(train.y[train.y == 1])
train.y.b = length(train.y[train.y == 0])
preds.num.s / (preds.num.b + preds.num.s)
train.y.s / (train.y.s / train.y.b)
train.y.s / (train.y.s / train.y.b)
train.y.s / (train.y.s + train.y.b)
library(xgboost)
library(caret)
setwd("~/GitHub-kszela24/Higgs")
#Reading in training and testing data.
train = read.csv("training.csv")
test = read.csv("test.csv")
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
#Setting the undefined values to NA
train[train == -999] = NA
test[test == -999] = NA
eval_met = "auc"
#Removing the training weight and trainig and testing eventID.
#Don't currently know what to do with the weight, so removing it preliminarily.
#Need to remove the training and testing eventID so that we don't train and predict on them.
train.ID <- train$EventId
train$EventId = NULL
test.ID <- test$EventId
test$EventId <- NULL
train.weight <- train$Weight
train$Weight = NULL
#Creating a vector for our target variable.
train.y <- train$Label
#Preparing for the training of the xgboost model.
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
watchlist <- list(train=dtrain)
#Creating the params and training the model.
#Initial parameters are copied from a Santander Kaggle script, going to do a grid search eventually
#with cross validation to find the best parameters.
#We are using a binary:logistic objective since this is a binary classification problem.
#We are using gbtree because it generally provides better results than other boosters.
#Changed from 560 nrounds to 200 nrounds because the AUC seemed to level off at that stage and the
#model may likely be overfitting (all preliminary).
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = eval_met,
eta                 = 0.02,
max_depth           = 5,
subsample           = 0.6815,
colsample_bytree    = 0.701
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 200,
nfolds              = 5,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
#Creating a sparse model matrix so that we can predict using the xgboost model we just trained.
test$Label <- -1
test <- sparse.model.matrix(Label ~ ., data = test)
preds <- predict(clf, test)
preds.num.s = length(preds[preds > 0.5])
preds.num.b = length(preds[preds <= 0.5])
preds.num.s / (preds.num.b + preds.num.s)
train.y.s = length(train.y[train.y == 1])
train.y.b = length(train.y[train.y == 0])
train.y.s / (train.y.s + train.y.b)
set.seed(1234)
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = eval_met,
eta                 = 0.02,
max_depth           = 5,
subsample           = 0.6815,
colsample_bytree    = 0.701
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 200,
nfolds              = 5,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
test = read.csv("test.csv")
test[test == -999] = NA
test.ID <- test$EventId
test$EventId <- NULL
test$Label <- -1
test <- sparse.model.matrix(Label ~ ., data = test)
preds <- predict(clf, test)
#Initial validation that we get roughly the same proportion of s's and b's in both the training
# and the test sets
preds.num.s = length(preds[preds > 0.5])
preds.num.b = length(preds[preds <= 0.5])
preds.num.s / (preds.num.b + preds.num.s)
train.y.s = length(train.y[train.y == 1])
train.y.b = length(train.y[train.y == 0])
train.y.s / (train.y.s + train.y.b)
library(xgboost)
library(caret)
setwd("~/GitHub-kszela24/Higgs")
#Reading in training and testing data.
train = read.csv("training.csv")
test = read.csv("test.csv")
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
#Setting the undefined values to NA, and let xgboost deal with them.
train[train == -999] = NA
test[test == -999] = NA
#Creating our first engineered feature!  Count the NAs in a row.
#Setting the evaluation metric for the xgboost to AUC to begin with, although others may be better
eval_met = "auc"
#Removing the training weight and trainig and testing eventID.
#Don't currently know what to do with the weight, so removing it preliminarily.
#Need to remove the training and testing eventID so that we don't train and predict on them.
train.ID <- train$EventId
train$EventId = NULL
test.ID <- test$EventId
test$EventId <- NULL
train.weight <- train$Weight
train$Weight = NULL
#Creating a vector for our target variable.
train.y <- train$Label
#Preparing for the training of the xgboost model.
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
watchlist <- list(train=dtrain)
#Creating the params and training the model.
#Initial parameters are copied from a Santander Kaggle script, going to do a grid search eventually
#with cross validation to find the best parameters.
#We are using a binary:logistic objective since this is a binary classification problem.
#We are using gbtree because it generally provides better results than other boosters.
#Changed from 560 nrounds to 200 nrounds because the AUC seemed to level off at that stage and the
#model may likely be overfitting (all preliminary).
#Set seed to 1234 for reproducibility (DONT FORGET TO DO THIS FOR YOUR OWN MODELS).
set.seed(0)
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = eval_met,
eta                 = 0.02,
max_depth           = 5,
subsample           = 0.6815,
colsample_bytree    = 0.701
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 200,
nfolds              = 5,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
#Creating a sparse model matrix so that we can predict using the xgboost model we just trained.
test$Label <- -1
test <- sparse.model.matrix(Label ~ ., data = test)
preds <- predict(clf, test)
#Initial validation that we get roughly the same proportion of s's and b's in both the training
# and the test sets
preds.num.s = length(preds[preds > 0.5])
preds.num.b = length(preds[preds <= 0.5])
preds.num.s / (preds.num.b + preds.num.s)
train.y.s = length(train.y[train.y == 1])
train.y.b = length(train.y[train.y == 0])
train.y.s / (train.y.s + train.y.b)
?xgboost
library(xgboost)
library(caret)
setwd("~/GitHub-kszela24/Higgs")
#Reading in training and testing data.
train = read.csv("training.csv")
test = read.csv("test.csv")
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
train$counts = apply(train,1,function(x) sum(is.na(x[1:33])))
train[train == -999] = NA
test[test == -999] = NA
#Creating our first engineered feature!  Count the NAs in a row.
train$counts = apply(train,1,function(x) sum(is.na(x[1:33])))
test$count = apply(train, 1, function(x) sum(is.na(x[1:31])))
test$count = apply(test, 1, function(x) sum(is.na(x[1:31])))
unique(train$counts)
library(xgboost)
library(caret)
setwd("~/GitHub-kszela24/Higgs")
#Reading in training and testing data.
train = read.csv("training.csv")
test = read.csv("test.csv")
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
#Setting the undefined values to NA, and letting xgboost deal with them.
train[train == -999] = NA
test[test == -999] = NA
#Creating our first engineered feature!  Count the NAs in a row.
train$counts = apply(train,1,function(x) sum(is.na(x[1:33])))
test$count = apply(test, 1, function(x) sum(is.na(x[1:31])))
unique(test$counts)
unique(test$count)
unique(train$counts)
setwd("~/GitHub-kszela24/higgs-bozon/Szela")
#Reading in training and testing data.
train = read.csv("training.csv")
test = read.csv("test.csv")
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
#Setting the undefined values to NA, and letting xgboost deal with them.
train[train == -999] = NA
test[test == -999] = NA
#Creating our first engineered feature!  Count the NAs in a row.
train$counts = apply(train, 1, function(x) sum(is.na(x[1:33])))
test$counts = apply(test, 1, function(x) sum(is.na(x[1:31])))
#Setting the evaluation metric for the xgboost to AUC to begin with, although others may be better
eval_met = "auc"
#Removing the training weight and trainig and testing eventID.
#Don't currently know what to do with the weight, so removing it preliminarily.
#Need to remove the training and testing eventID so that we don't train and predict on them.
train.ID <- train$EventId
train$EventId = NULL
test.ID <- test$EventId
test$EventId <- NULL
train.weight <- train$Weight
train$Weight = NULL
#Creating a vector for our target variable.
train.y <- train$Label
#Preparing for the training of the xgboost model.
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
watchlist <- list(train=dtrain)
#Creating the params and training the model.
#Initial parameters are copied from a Santander Kaggle script, going to do a grid search eventually
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
#Loading important packages.
library(xgboost)
library(caret)
setwd("~/GitHub-kszela24/higgs-bozon/Szela")
#Reading in training and testing data.
train = read.csv("training.csv")
test = read.csv("test.csv")
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
#Setting the undefined values to NA, and letting xgboost deal with them.
train[train == -999] = NA
test[test == -999] = NA
#Creating our first engineered feature!  Count the NAs in a row.
train$counts = apply(train, 1, function(x) sum(is.na(x[1:33])))
test$counts = apply(test, 1, function(x) sum(is.na(x[1:31])))
#Setting the evaluation metric for the xgboost to AUC to begin with, although others may be better
eval_met = "auc"
#Removing the training weight and trainig and testing eventID.
#Don't currently know what to do with the weight, so removing it preliminarily.
#Need to remove the training and testing eventID so that we don't train and predict on them.
train.ID <- train$EventId
train$EventId = NULL
test.ID <- test$EventId
test$EventId <- NULL
train.weight <- train$Weight
train$Weight = NULL
#Creating a vector for our target variable.
train.y <- train$Label
#Preparing for the training of the xgboost model.
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
?read.csv
library(xgboost)
library(caret)
setwd("~/GitHub-kszela24/higgs-bozon/Szela")
#Reading in training and testing data.
train = as.data.frame(read.csv("training.csv"))
test = as.data.frame(read.csv("test.csv"))
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
#Setting the undefined values to NA, and letting xgboost deal with them.
train[train == -999] = NA
test[test == -999] = NA
#Creating our first engineered feature!  Count the NAs in a row.
train$countNA = apply(train, 1, function(x) sum(is.na(x[1:33])))
test$countNA = apply(test, 1, function(x) sum(is.na(x[1:31])))
#Setting the evaluation metric for the xgboost to AUC to begin with, although others may be better
eval_met = "auc"
#Removing the training weight and trainig and testing eventID.
#Don't currently know what to do with the weight, so removing it preliminarily.
#Need to remove the training and testing eventID so that we don't train and predict on them.
train.ID <- train$EventId
train$EventId = NULL
test.ID <- test$EventId
test$EventId <- NULL
train.weight <- train$Weight
train$Weight = NULL
#Creating a vector for our target variable.
train.y <- train$Label
#Preparing for the training of the xgboost model.
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
train.y <- as.vector(train$Label)
#Preparing for the training of the xgboost model.
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
train.model <- sparse.model.matrix(Label ~ ., data = train)[,-1]
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
train.model <- sparse.model.matrix(Label ~ ., data = train)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
length(train.y)
library(xgboost)
library(caret)
setwd("~/GitHub-kszela24/higgs-bozon/Szela")
#Reading in training and testing data.
train = as.data.frame(read.csv("training.csv"))
test = as.data.frame(read.csv("test.csv"))
#Releveling b and s such that b = 0 and s = 1, so that we can do logistic regression on them.
levels(train$Label) = c(0, 1)
train$Label = as.numeric(train$Label) - 1
#Creating a vector for our target variable.
train.y <- as.vector(train$Label)
train$Label = NULL
#Setting the undefined values to NA, and letting xgboost deal with them.
train[train == -999] = NA
test[test == -999] = NA
#Creating our first engineered feature!  Count the NAs in a row.
train$countNA = apply(train, 1, function(x) sum(is.na(x[1:33])))
test$countNA = apply(test, 1, function(x) sum(is.na(x[1:31])))
#Setting the evaluation metric for the xgboost to AUC to begin with, although others may be better
eval_met = "auc"
#Removing the training weight and trainig and testing eventID.
#Don't currently know what to do with the weight, so removing it preliminarily.
#Need to remove the training and testing eventID so that we don't train and predict on them.
train.ID <- train$EventId
train$EventId = NULL
test.ID <- test$EventId
test$EventId <- NULL
train.weight <- train$Weight
train$Weight = NULL
train$TARGET = train.y
train.model <- sparse.model.matrix(TARGET ~ ., data = train)
length(train.y)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
?xgb.DMatrix
?sparse.model.matrix
train.model <- sparse.model.matrix(TARGET ~ ., data = as.data.frame(train))
length(train.y)
dtrain <- xgb.DMatrix(data = train.model, label = train.y)
cat("\n## Removing the constants features.\n")
for (f in names(train)) {
if (length(unique(train[[f]])) == 1) {
cat(f, "is constant in train. We delete it.\n")
train[[f]] <- NULL
test[[f]] <- NULL
}
}
